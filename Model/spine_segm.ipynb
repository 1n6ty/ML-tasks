{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spine Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "\n",
    "from random import randint\n",
    "from PIL import Image\n",
    "import os\n",
    "import pydicom\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import Sequence\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init global variables\n",
    "DATA_DIR = os.path.abspath('../Data/spine-segmentation')\n",
    "X_FILES = [os.path.abspath(os.path.join(DATA_DIR, f'dicom/{i}')) for i in os.listdir(os.path.join(DATA_DIR, 'dicom'))]\n",
    "Y_FILES = [os.path.abspath(os.path.join(DATA_DIR, f'filled/{i}')) for i in os.listdir(os.path.join(DATA_DIR, 'filled'))]\n",
    "\n",
    "RESULTS = os.path.abspath('../Results')\n",
    "\n",
    "IMG_SHAPE = (576, 240)\n",
    "VALIDATION_NUM = 6\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_train_generator(Sequence):\n",
    "    def __init__(self, x_files_list: list, y_files_list: list, batch_size: int, new_image_size = None, shuffle = True) -> None:\n",
    "        self.data = x_files_list\n",
    "        self.labels = y_files_list\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.shuffle = shuffle\n",
    "        self.new_image_size = new_image_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / float(self.batch_size)))\n",
    "\n",
    "    def __open_png_y(self, file_path):\n",
    "        img = Image.open(file_path)\n",
    "        data = np.array(img, dtype=\"float32\")\n",
    "        \n",
    "        if self.new_image_size != None:\n",
    "            data = cv2.resize(data[:4608, :1920], dsize=self.new_image_size[::-1], interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        img.close()\n",
    "        return np.reshape(data, (*self.new_image_size, 1))\n",
    "\n",
    "    def __open_dcm_x(self, file_path):\n",
    "        dcm = pydicom.dcmread(file_path)\n",
    "\n",
    "        data = dcm.pixel_array.astype(\"float32\") / 255\n",
    "\n",
    "        if self.new_image_size != None:\n",
    "            data = cv2.resize(data[:4608, :1920], dsize=self.new_image_size[::-1], interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        return np.reshape(data, (*self.new_image_size, 1))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_x = np.array(list(map(self.__open_dcm_x, self.data[index * self.batch_size: (index + 1) * self.batch_size])))\n",
    "        batch_y = np.array(list(map(self.__open_png_y, self.labels[index * self.batch_size: (index + 1) * self.batch_size])))\n",
    "\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            for i in range(len(self.data)):\n",
    "                ind_from, ind_to = randint(0, len(self.data) - 1), randint(0, len(self.data) - 1)\n",
    "                self.data[ind_from], self.data[ind_to] = self.data[ind_to], self.data[ind_from]\n",
    "                self.labels[ind_from], self.labels[ind_to] = self.labels[ind_to], self.labels[ind_from]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training And Validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def make_dice_loss(smooth=1e-6, gama=2):\n",
    "    def dice_loss(y_true, y_pred):\n",
    "        y_true, y_pred = tf.cast(y_true, dtype=tf.float32), tf.cast(y_pred, dtype=tf.float32)\n",
    "        nominator = 2 * tf.reduce_sum(tf.multiply(y_pred, y_true)) + smooth\n",
    "        denominator = tf.reduce_sum(y_pred ** gama) + tf.reduce_sum(y_true ** gama) + smooth\n",
    "        return 1 - tf.divide(nominator, denominator)\n",
    "    return dice_loss\n",
    "\n",
    "data_gen = Data_train_generator(X_FILES[:-VALIDATION_NUM], Y_FILES[:-VALIDATION_NUM], BATCH_SIZE, IMG_SHAPE)\n",
    "val_gen = Data_train_generator(X_FILES[-VALIDATION_NUM:], Y_FILES[-VALIDATION_NUM:], BATCH_SIZE, IMG_SHAPE)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    filepath=os.path.join(RESULTS, 'saved_weights/{epoch}-weights_unetpp.hdf5')\n",
    ")\n",
    "\n",
    "from model import make_UNet3p\n",
    "\n",
    "model_unet = make_UNet3p((*IMG_SHAPE, 1), [64, 128, 256, 512, 1024, 320, 320, 320, 320], deep_supervision=True)\n",
    "\n",
    "model_unet.compile(optimizer='Adam', loss={\n",
    "    'output_1': make_dice_loss(),\n",
    "    'output_2': make_dice_loss(),\n",
    "    'output_3': make_dice_loss(),\n",
    "    'output_4': make_dice_loss(),\n",
    "    'output_5': make_dice_loss()\n",
    "}, loss_weights=[1.0, 1.0, 1.0, 1.0, 1.0])\n",
    "\n",
    "history_unet = model_unet.fit(x=data_gen, epochs=20, validation_data=val_gen, callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "WEIGHTS2LOAD = os.path.join(RESULTS, 'saved_weights/1-weights_unetpp.hdf5')\n",
    "\n",
    "TEST_IMG_PATH = os.path.join(DATA_DIR, 'dicom/050_SD.dcm')\n",
    "TEST_TRUE_IMG_PATH = os.path.join(DATA_DIR, 'filled/050_SD.png')\n",
    "\n",
    "test_gen = Data_train_generator([TEST_IMG_PATH], [TEST_TRUE_IMG_PATH], batch_size=1, new_image_size=IMG_SHAPE)\n",
    "\n",
    "res = model_unet.predict(test_gen.__getitem__(0)[0], batch_size=1)\n",
    "res = (res[0] + res[1] + res[2] + res[3]) / 4\n",
    "res = res.reshape(IMG_SHAPE)\n",
    "\n",
    "# Drawing Result\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=4)\n",
    "fig.set_figwidth(12)\n",
    "ax[3].set_title('mask')\n",
    "ax[1].set_title('prediction')\n",
    "\n",
    "sns.heatmap(res, ax=ax[0])\n",
    "sns.heatmap(np.where(res > 0.5, 1, 0), ax=ax[1])\n",
    "sns.heatmap(test_gen.__getitem__(0)[0].reshape(IMG_SHAPE), ax=ax[2])\n",
    "sns.heatmap(test_gen.__getitem__(0)[1].reshape(IMG_SHAPE), ax=ax[3])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "b135ae7acc43b5801ffcf4e47b4a48a7af7b1a21cf3a8568b85aad59033717de"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
