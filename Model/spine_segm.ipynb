{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spine Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from random import randint\n",
    "import cv2\n",
    "import os\n",
    "import pydicom\n",
    "import json\n",
    "\n",
    "from func import search_for_borders, get_edgefunc_coef, get_vertebras_corners, get_ratio, approximateYbezie_lcr, adjast_vertebras, MSE\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init global variables\n",
    "\n",
    "DATA_DIR = os.path.abspath('../Data/spine-segmentation')\n",
    "with open(os.path.join(DATA_DIR, 'files_routes.json'), 'r') as f:\n",
    "    FILE_DIRS = json.loads(f.read())\n",
    "\n",
    "RESULTS = os.path.abspath('../Results')\n",
    "\n",
    "IMG_SHAPE = (3408, 1552)\n",
    "VALIDATION_NUM = 1\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "WEIGHTS2LOAD = os.path.join(RESULTS, 'saved_weights/10-weights_unetpp.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generator\n",
    "\n",
    "class Data_train_generator(Sequence):\n",
    "    def __init__(self, x_files_list: list, y_files_list: list, batch_size: int, new_image_size = None, shuffle = True) -> None:\n",
    "        self.data = x_files_list\n",
    "        self.labels = y_files_list\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.shuffle = shuffle\n",
    "        self.new_image_size = new_image_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / float(self.batch_size)))\n",
    "\n",
    "    def __open_png_y(self, file_path):\n",
    "        img = cv2.imread(file_path)\n",
    "        img = cv2.inRange(img, (20, 20, 210), (40, 40, 240))\n",
    "        \n",
    "        data = np.array(img, dtype=\"float32\") / 255\n",
    "\n",
    "        if self.new_image_size != None:\n",
    "            data = cv2.resize(data, self.new_image_size[::-1], interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        return np.reshape(data, (*self.new_image_size, 1))\n",
    "\n",
    "    def __open_dcm_x(self, file_path):\n",
    "        dcm = pydicom.dcmread(file_path)\n",
    "\n",
    "        data = dcm.pixel_array.astype(\"float32\") / 255\n",
    "\n",
    "        if self.new_image_size != None:\n",
    "            data = cv2.resize(data, self.new_image_size[::-1], interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        return np.reshape(data, (*self.new_image_size, 1))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_x = np.array(list(map(self.__open_dcm_x, self.data[index * self.batch_size: (index + 1) * self.batch_size])))\n",
    "        batch_y = np.array(list(map(self.__open_png_y, self.labels[index * self.batch_size: (index + 1) * self.batch_size])))\n",
    "\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            for i in range(len(self.data)):\n",
    "                ind_from, ind_to = randint(0, len(self.data) - 1), randint(0, len(self.data) - 1)\n",
    "                self.data[ind_from], self.data[ind_to] = self.data[ind_to], self.data[ind_from]\n",
    "                self.labels[ind_from], self.labels[ind_to] = self.labels[ind_to], self.labels[ind_from]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model Init\n",
    "\n",
    "from model import make_unet2p\n",
    "\n",
    "model_unet = make_unet2p((*IMG_SHAPE, 1), filters=[64, 128, 256, 512, 1024], deep_supervision=True)\n",
    "\n",
    "if WEIGHTS2LOAD: model_unet.load_weights(WEIGHTS2LOAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "dcm = pydicom.dcmread(os.path.join(DATA_DIR, 'IMG-0001-00002.dcm'))\n",
    "data = dcm.pixel_array.astype(\"float32\") / 255\n",
    "\n",
    "if IMG_SHAPE != None:\n",
    "    data = cv2.resize(data, IMG_SHAPE[::-1], interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "data = np.reshape(data, (1, *IMG_SHAPE, 1))\n",
    "\n",
    "res = model_unet.predict(data, batch_size=1)[3].reshape(IMG_SHAPE)\n",
    "\n",
    "# Drawing Result\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "sns.heatmap(res, ax=ax[0])\n",
    "sns.heatmap(np.where(res > 0.5, 1, 0), ax=ax[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training To Point Vertebras\n",
    "\n",
    "IMG_PATH = os.path.join(DATA_DIR, 'test/IMG-0001-00001.png')\n",
    "\n",
    "img = cv2.inRange(cv2.imread(IMG_PATH), (20, 20, 210), (40, 40, 240))\n",
    "\n",
    "data = np.array(img, dtype=\"float32\") / 255\n",
    "\n",
    "n = 10\n",
    "threshold = 0.7\n",
    "\n",
    "border_coords = search_for_borders(data)\n",
    "\n",
    "edge_coef = get_edgefunc_coef(data, border_coords, n)\n",
    "\n",
    "y, y_l, y_r = approximateYbezie_lcr(border_coords, edge_coef, n)\n",
    "\n",
    "vertebras_corners = get_vertebras_corners(data, border_coords, y, y_l, y_r, threshold)\n",
    "\n",
    "#vertebras_corners = adjast_vertebras(border_coords, vertebras_corners, y, y_l, y_r)\n",
    "#vertebras_corners_true = adjast_vertebras(border_coords_true, vertebras_corners_true, y_true, y_l_true, y_r_true) \n",
    "# It can't be used due to a huge size of S1 part\n",
    "\n",
    "#error = MSE(vertebras_corners, vertebras_corners_true)\n",
    "\n",
    "# Point Vertebras\n",
    "\n",
    "vertebras_corners = [[[vertebras_corners[i], vertebras_corners[i + 1]], [vertebras_corners[i + 2], vertebras_corners[i + 3]]] for i in range(0, len(vertebras_corners), 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting Info about training\n",
    "\n",
    "with open(os.path.join(RESULTS, 'model_history_1'), 'rb') as f:\n",
    "    hist = pickle.load(f)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "sns.lineplot(hist[\"loss\"], ax=ax[0][0])\n",
    "sns.lineplot(hist[\"val_loss\"], ax=ax[0][1])\n",
    "\n",
    "sns.lineplot(hist[\"output_4_loss\"], ax=ax[1][0])\n",
    "sns.lineplot(hist[\"val_output_4_loss\"], ax=ax[1][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "380bfe0694a67091c1b1fc5ff54d9e5aa4d3b44503f6eeeca4918ab48e068b14"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
