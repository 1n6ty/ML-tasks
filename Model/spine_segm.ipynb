{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "\n",
    "from random import randint\n",
    "from PIL import Image\n",
    "from itertools import product\n",
    "import os\n",
    "import pydicom\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras.layers import Concatenate, Conv2D, Conv2DTranspose, MaxPool2D, Input, Activation, BatchNormalization, Dropout, concatenate\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init global variables\n",
    "X_TRAIN_DIR = os.path.abspath('x_train')\n",
    "Y_TRAIN_DIR = os.path.abspath('y_train')\n",
    "\n",
    "X_VAL_DIR = os.path.abspath('x_val')\n",
    "Y_VAL_DIR = os.path.abspath('y_val')\n",
    "\n",
    "SAVED_WEIGHTS = os.path.abspath('saved_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building data generators\n",
    "class Data_train_generator(Sequence):\n",
    "    def __init__(self, data_dir, y_true_dir, batch_size, new_img_size: tuple[int, int]) -> None:\n",
    "        self.data = os.listdir(data_dir)\n",
    "        self.labels = os.listdir(y_true_dir)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.y_true_dir = y_true_dir\n",
    "\n",
    "        self.new_img_size = new_img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / float(self.batch_size)))\n",
    "\n",
    "    def __open_png_y(self, file_path):\n",
    "        img = Image.open(os.path.join(self.y_true_dir, file_path))\n",
    "        data = np.array(img, dtype=\"float32\")\n",
    "        data = cv2.resize(data[:4608, :1920], dsize=self.new_img_size[::-1], interpolation=cv2.INTER_AREA)\n",
    "        img.close()\n",
    "        return data.reshape((*self.new_img_size, 1))\n",
    "\n",
    "    def __open_dcm_x(self, file_path):\n",
    "        dcm = pydicom.dcmread(os.path.join(self.data_dir, file_path))\n",
    "\n",
    "        d = dcm.pixel_array.astype(\"float32\") / 255\n",
    "        d = cv2.resize(d[:4608, :1920], dsize=self.new_img_size[::-1], interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        return d.reshape((*self.new_img_size, 1))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_x = np.array(list(map(self.__open_dcm_x, self.data[index * self.batch_size: (index + 1) * self.batch_size])))\n",
    "        batch_y = np.array(list(map(self.__open_png_y, self.labels[index * self.batch_size: (index + 1) * self.batch_size])))\n",
    "\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        for i in range(len(self.data)):\n",
    "            ind_from, ind_to = randint(0, len(self.data) - 1), randint(0, len(self.data) - 1)\n",
    "            self.data[ind_from], self.data[ind_to] = self.data[ind_to], self.data[ind_from]\n",
    "            self.labels[ind_from], self.labels[ind_to] = self.labels[ind_to], self.labels[ind_from]\n",
    "\n",
    "class Test_train_generator(Sequence):\n",
    "    def __init__(self, data_file, y_file, data_dir, y_true_dir, batch_size, new_img_size: tuple[int, int]) -> None:\n",
    "        self.data = [data_file for i in range(500)]\n",
    "        self.labels = [y_file for i in range(500)]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.y_true_dir = y_true_dir\n",
    "\n",
    "        self.new_img_size = new_img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / float(self.batch_size)))\n",
    "\n",
    "    def __open_png_y(self, file_path):\n",
    "        img = Image.open(os.path.join(self.y_true_dir, file_path))\n",
    "        data = np.array(img, dtype=\"float32\")\n",
    "        data = cv2.resize(data[:4608, :1920], dsize=self.new_img_size[::-1], interpolation=cv2.INTER_AREA)\n",
    "        img.close()\n",
    "        return data.reshape((*self.new_img_size, 1))\n",
    "\n",
    "    def __open_dcm_x(self, file_path):\n",
    "        dcm = pydicom.dcmread(os.path.join(self.data_dir, file_path))\n",
    "\n",
    "        d = dcm.pixel_array.astype(\"float32\") / 255\n",
    "        d = cv2.resize(d[:4608, :1920], dsize=self.new_img_size[::-1], interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        return d.reshape((*self.new_img_size, 1))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_x = np.array(list(map(self.__open_dcm_x, self.data[index * self.batch_size: (index + 1) * self.batch_size])))\n",
    "        batch_y = np.array(list(map(self.__open_png_y, self.labels[index * self.batch_size: (index + 1) * self.batch_size])))\n",
    "\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        for i in range(len(self.data)):\n",
    "            ind_from, ind_to = randint(0, len(self.data) - 1), randint(0, len(self.data) - 1)\n",
    "            self.data[ind_from], self.data[ind_to] = self.data[ind_to], self.data[ind_from]\n",
    "            self.labels[ind_from], self.labels[ind_to] = self.labels[ind_to], self.labels[ind_from]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.1\n",
    "def standard_unit(input_tensor, stage, nb_filter, kernel_size=3):\n",
    "\n",
    "    act = 'elu'\n",
    "\n",
    "    x = Conv2D(nb_filter, (kernel_size, kernel_size), activation=act, name='conv'+stage+'_1', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(input_tensor)\n",
    "    x = Dropout(dropout_rate, name='dp'+stage+'_1')(x)\n",
    "    x = Conv2D(nb_filter, (kernel_size, kernel_size), activation=act, name='conv'+stage+'_2', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(x)\n",
    "    x = Dropout(dropout_rate, name='dp'+stage+'_2')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def Nest_Net(img_rows, img_cols, color_type=1, num_class=1, deep_supervision=False):\n",
    "\n",
    "    nb_filter = [32,64,128,256,512]\n",
    "    act = 'elu'\n",
    "\n",
    "    bn_axis = 3\n",
    "    img_input = Input(shape=(img_rows, img_cols, color_type), name='main_input')\n",
    "\n",
    "    conv1_1 = standard_unit(img_input, stage='11', nb_filter=nb_filter[0])\n",
    "    pool1 = MaxPool2D((2, 2), strides=(2, 2), name='pool1')(conv1_1)\n",
    "\n",
    "    conv2_1 = standard_unit(pool1, stage='21', nb_filter=nb_filter[1])\n",
    "    pool2 = MaxPool2D((2, 2), strides=(2, 2), name='pool2')(conv2_1)\n",
    "\n",
    "    up1_2 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up12', padding='same')(conv2_1)\n",
    "    conv1_2 = concatenate([up1_2, conv1_1], name='merge12', axis=bn_axis)\n",
    "    conv1_2 = standard_unit(conv1_2, stage='12', nb_filter=nb_filter[0])\n",
    "\n",
    "    conv3_1 = standard_unit(pool2, stage='31', nb_filter=nb_filter[2])\n",
    "    pool3 = MaxPool2D((2, 2), strides=(2, 2), name='pool3')(conv3_1)\n",
    "\n",
    "    up2_2 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up22', padding='same')(conv3_1)\n",
    "    conv2_2 = concatenate([up2_2, conv2_1], name='merge22', axis=bn_axis)\n",
    "    conv2_2 = standard_unit(conv2_2, stage='22', nb_filter=nb_filter[1])\n",
    "\n",
    "    up1_3 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up13', padding='same')(conv2_2)\n",
    "    conv1_3 = concatenate([up1_3, conv1_1, conv1_2], name='merge13', axis=bn_axis)\n",
    "    conv1_3 = standard_unit(conv1_3, stage='13', nb_filter=nb_filter[0])\n",
    "\n",
    "    conv4_1 = standard_unit(pool3, stage='41', nb_filter=nb_filter[3])\n",
    "    pool4 = MaxPool2D((2, 2), strides=(2, 2), name='pool4')(conv4_1)\n",
    "\n",
    "    up3_2 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up32', padding='same')(conv4_1)\n",
    "    conv3_2 = concatenate([up3_2, conv3_1], name='merge32', axis=bn_axis)\n",
    "    conv3_2 = standard_unit(conv3_2, stage='32', nb_filter=nb_filter[2])\n",
    "\n",
    "    up2_3 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up23', padding='same')(conv3_2)\n",
    "    conv2_3 = concatenate([up2_3, conv2_1, conv2_2], name='merge23', axis=bn_axis)\n",
    "    conv2_3 = standard_unit(conv2_3, stage='23', nb_filter=nb_filter[1])\n",
    "\n",
    "    up1_4 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up14', padding='same')(conv2_3)\n",
    "    conv1_4 = concatenate([up1_4, conv1_1, conv1_2, conv1_3], name='merge14', axis=bn_axis)\n",
    "    conv1_4 = standard_unit(conv1_4, stage='14', nb_filter=nb_filter[0])\n",
    "\n",
    "    conv5_1 = standard_unit(pool4, stage='51', nb_filter=nb_filter[4])\n",
    "\n",
    "    up4_2 = Conv2DTranspose(nb_filter[3], (2, 2), strides=(2, 2), name='up42', padding='same')(conv5_1)\n",
    "    conv4_2 = concatenate([up4_2, conv4_1], name='merge42', axis=bn_axis)\n",
    "    conv4_2 = standard_unit(conv4_2, stage='42', nb_filter=nb_filter[3])\n",
    "\n",
    "    up3_3 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up33', padding='same')(conv4_2)\n",
    "    conv3_3 = concatenate([up3_3, conv3_1, conv3_2], name='merge33', axis=bn_axis)\n",
    "    conv3_3 = standard_unit(conv3_3, stage='33', nb_filter=nb_filter[2])\n",
    "    up2_4 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up24', padding='same')(conv3_3)\n",
    "    conv2_4 = concatenate([up2_4, conv2_1, conv2_2, conv2_3], name='merge24', axis=bn_axis)\n",
    "    conv2_4 = standard_unit(conv2_4, stage='24', nb_filter=nb_filter[1])\n",
    "\n",
    "    up1_5 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up15', padding='same')(conv2_4)\n",
    "    conv1_5 = concatenate([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], name='merge15', axis=bn_axis)\n",
    "    conv1_5 = standard_unit(conv1_5, stage='15', nb_filter=nb_filter[0])\n",
    "\n",
    "    nestnet_output_1 = Conv2D(num_class, (1, 1), activation='sigmoid', name='output_1', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_2)\n",
    "    nestnet_output_2 = Conv2D(num_class, (1, 1), activation='sigmoid', name='output_2', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_3)\n",
    "    nestnet_output_3 = Conv2D(num_class, (1, 1), activation='sigmoid', name='output_3', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_4)\n",
    "    nestnet_output_4 = Conv2D(num_class, (1, 1), activation='sigmoid', name='output_4', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_5)\n",
    "\n",
    "    if deep_supervision:\n",
    "        model = Model(img_input, [nestnet_output_1,nestnet_output_2,nestnet_output_3,nestnet_output_4])\n",
    "    else:\n",
    "        model = Model(img_input, [nestnet_output_4])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def make_dice_loss(smooth=1e-6, gama=2):\n",
    "    def dice_loss(y_true, y_pred):\n",
    "        y_true, y_pred = tf.cast(y_true, dtype=tf.float32), tf.cast(y_pred, tf.float32)\n",
    "        nominator = 2 * tf.reduce_sum(tf.multiply(y_pred, y_true)) + smooth\n",
    "        denominator = tf.reduce_sum(y_pred ** gama) + tf.reduce_sum(y_true ** gama) + smooth\n",
    "        return 1 - tf.divide(nominator, denominator)\n",
    "    return dice_loss\n",
    "\n",
    "data_gen = Data_train_generator(X_TRAIN_DIR, Y_TRAIN_DIR, 16, (576, 240))\n",
    "val_gen = Data_train_generator(X_VAL_DIR, Y_VAL_DIR, 16, (576, 240))\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    filepath='./saved_weights/{val_loss:.3f}-weights_unetpp.hdf5'\n",
    ")\n",
    "\n",
    "model_unet = Nest_Net(576, 240, color_type=1, num_class=1, deep_supervision=True)\n",
    "\n",
    "model_unet.compile(optimizer='Adam', loss={\n",
    "    'output_1': make_dice_loss(),\n",
    "    'output_2': make_dice_loss(),\n",
    "    'output_3': make_dice_loss(),\n",
    "    'output_4': make_dice_loss()\n",
    "}, loss_weights=[1.0, 1.0, 1.0, 1.0])\n",
    "\n",
    "history_unet = model_unet.fit(x=data_gen, epochs=20, validation_data=val_gen, callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "test_img, test_true_img = Test_train_generator('046_SD.dcm', '046_SD.png', X_VAL_DIR, Y_VAL_DIR, 16, (576, 240)).__getitem__(0)\n",
    "test_img, test_true_img = test_img[3].reshape((1, 576, 240, 1)), test_true_img[0]\n",
    "\n",
    "res = model_unet.predict(test_img, batch_size=1)\n",
    "res = (res[0] + res[1] + res[2] + res[3]) / 4\n",
    "res = res.reshape((576, 240))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=4)\n",
    "fig.set_figwidth(12)\n",
    "ax[3].set_title('mask')\n",
    "ax[1].set_title('prediction')\n",
    "\n",
    "sns.heatmap(res, ax=ax[0])\n",
    "sns.heatmap(np.where(res > 0.5, 1, 0), ax=ax[1])\n",
    "sns.heatmap(test_img.reshape((576, 240)), ax=ax[2])\n",
    "sns.heatmap(test_true_img.reshape((576, 240)), ax=ax[3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "b135ae7acc43b5801ffcf4e47b4a48a7af7b1a21cf3a8568b85aad59033717de"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
